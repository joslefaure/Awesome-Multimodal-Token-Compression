<div align=center>

# Awesome Multimodal Token Compression

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)
[![arXiv](https://img.shields.io/badge/arXiv-2507\.20198-red.svg)](https://arxiv.org/abs/2507.20198)
[![Last Commit](https://img.shields.io/github/last-commit/cokeshao/Awesome-Multimodal-Token-Compression.svg?style=flat&color=orange)](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression)

[[arXiv]](https://arxiv.org/abs/2507.20198) [[HuggingFace]](https://huggingface.co/papers/2507.20198) [[Database]](https://oasis-paddleboat-fc1.notion.site/when-tokens-talk-too-much-database)

</div>

> **When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios** [[arXiv]](https://arxiv.org/pdf/2507.20198)   
> [Kele Shao](https://cokeshao.github.io/)<sup>\*,1,2</sup>, [Keda Tao](https://kd-tao.github.io/)<sup>\*,1,2</sup>, [Kejia Zhang](https://kejiazhang-robust.github.io/)<sup>3</sup>, [Sicheng Feng](https://fscdc.github.io/)<sup>2,4</sup>, [Mu Cai](https://pages.cs.wisc.edu/~mucai/)<sup>5</sup>, [Yuzhang Shang](https://42shawn.github.io/)<sup>6</sup>, [Haoxuan You](https://hxyou.github.io/)<sup>7</sup>, [Can Qin](https://canqin.tech/)<sup>8</sup>, [Yang Sui](https://eclipsess.github.io/yangsui.github.io/)<sup>9</sup>, [Huan Wang](https://huanwang.tech/)<sup>‚Ä†,2</sup>
> 
> <sup>1</sup>Zhejiang University, <sup>2</sup>Westlake University, <sup>3</sup>Xiamen University, <sup>4</sup>National University of Singapore, <sup>5</sup>University of Wisconsin-Madison, <sup>6</sup>University of Central Florida, <sup>7</sup>Columbia University, <sup>8</sup>Salesforce AI Research, <sup>9</sup>Rice University
> 
> \* Equal Contribution.  ‚Ä† Corresponding Author (wanghuan@westlake.edu.cn).

---

> [!IMPORTANT]
> We welcome your help in improving the repository and paper. Please feel free to submit a [pull request](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/pulls) or [contact us](#Ô∏è-contact) to:
> 
> - Add a relevant paper not yet included.
>
> - Suggest a more suitable category.
>
> - Update the information.
>
> - Ask for clarification about any content.

---

## üî• News

- **[2025.08.14]** ‚ùó Added [Recent Papers](#recent-papers-last-6-months), [Papers Published in Recent Conference/Journal](#published-in-recent-conferencejournal), and a [database](https://oasis-paddleboat-fc1.notion.site/when-tokens-talk-too-much-database) for quick-search.
- **[2025.07.29]** The v1 survey is now published! We've also initialized the repository.

## üéØ Motivation
<div align="left">
  <img src="images/motivation.png" alt="Awesome Token Compression" width="400"/>
</div>

> **Motivation:** **Up:** Image, video, and audio data types can scale in their representation dimensions, leading to a corresponding increase in the number of tokens. **Down:** Top-performing MLLMs cannot address real-world demands, as the number of tokens for multimodal information, especially video, vastly exceeds that of text. Therefore, token compression is crucial to address this limitation.

## üìå Citation

If you find our paper or this resource helpful, please consider cite:

```bibtex
@article{shao2025tokens,
  title={When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios},
  author={Shao, Kele and Tao, Keda and Zhang, Kejia and Feng, Sicheng and Cai, Mu and Shang, Yuzhang and You, Haoxuan and Qin, Can and Sui, Yang and Wang, Huan},
  journal={arXiv preprint arXiv:2507.20198},
  year={2025}
}
```

## üìö Contents

- [Awesome Token Compression](#awesome-multimodal-token-compression)
    - [Image LLM](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/tree/main/image-llm.md)
    - [Video LLM](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/tree/main/video-llm.md)
    - [Audio LLM](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/tree/main/audio-llm.md)
    - [Vision Transformer](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/tree/main/vision-transformer.md)
    - [Audio Transformer](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/tree/main/audio-transformer.md)

**Please check out all the papers by selecting the sub-area you're interested in. On this main page, only papers released in the past 6 months are shown.**

---

### Badge Colors
- ![arXiv Badge](https://img.shields.io/badge/arXiv-red) `red` for arXiv papers
- ![PDF Badge](https://img.shields.io/badge/PDF-blue) `blue` for conference/journal papers
- ![GitHub Badge](https://img.shields.io/badge/GitHub-white) `white` for GitHub repositories
- ![Research Areas Badge](https://img.shields.io/badge/Areas-purple) `purple` for research areas
- ![Categories Badge](https://img.shields.io/badge/Categories-green) `green` for categories
- ![Cost Badge](https://img.shields.io/badge/Cost-yellow) `yellow` for training cost

### Recent Papers (Last 6 Months)


<details open>
<summary><strong>Image</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning](https://arxiv.org/abs/2508.07871)<br>Yanshu Li, Jianjiang Yang, Zhennan Shen, Ligong Han, Haoyan Xu, Ruixiang Tang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.07871)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)<br>Weichen Zhang, Zhui Zhu, Ningbo Li, Kebin Liu, Yunhao Liu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.06084)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)<br>Huanyu Wang, Jushi Kai, Haoli Bai, Lu Hou, Bo Jiang, Ziwei He, Zhouhan Lin |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.06038)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/sihany077/VFlowOpt.svg?style=social&label=Star)](https://github.com/https://github.com/sihany077/VFlowOpt)<br>[VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization](https://arxiv.org/abs/2508.05211)<br>Sihan Yang, Runsen Xu, Chenhang Cui, Tai Wang, Dahua Lin, Jiangmiao Pang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.05211)<br> [GitHub](https://github.com/sihany077/VFlowOpt)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() [![Star](https://img.shields.io/github/stars/HVision-NKU/GlimpsePrune.svg?style=social&label=Star)](https://github.com/https://github.com/HVision-NKU/GlimpsePrune)<br>[A Glimpse to Compress: Dynamic Visual Token Pruning for Large Vision-Language Models](https://arxiv.org/abs/2508.01548)<br>Quan-Sheng Zeng, Yunheng Li, Qilong Wang, Peng-Tao Jiang, Zuxuan Wu, Ming-Ming Cheng, Qibin Hou |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.01548)<br> [GitHub](https://github.com/HVision-NKU/GlimpsePrune)<br> [Model](https://huggingface.co/collections/ashun989/glimpseprune-688d8826ef5bd09db6af145e)<br> | 
|  [![Publish](https://img.shields.io/badge/ACM_MM-2025-blue)]() <br>[Mitigating Information Loss under High Pruning Rates for Efficient Large Vision Language Models](https://arxiv.org/abs/2508.01236)<br>Mingyu Fu, Wei Suo, Ji Ma, Lin Yuanbo Wu, Peng Wang, Yanning Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.01236)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models](https://arxiv.org/abs/2508.00553)<br>Jizhihui Liu, Feiyi Du, Guangdao Zhu, Niu Lian, Jun Li, Bin Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.00553)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.07-red)]() <br>[FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning](https://arxiv.org/abs/2507.23318)<br>Jiajun Cao, Qizhe Zhang, Peidong Jia, Xuhui Zhao, Bo Lan, Xiaoan Zhang, Xiaobao Wei, Sixiang Chen, Zhuo Li, Yang Wang, Liyun Li, Xianming Liu, Ming Lu, Shanghang Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2507.23318)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/YuchenLiu98/METEOR.svg?style=social&label=Star)](https://github.com/https://github.com/YuchenLiu98/METEOR)<br>[METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models](https://arxiv.org/abs/2507.20842)<br>Yuchen Liu, Yaoming Wang, Bowen Shi, Xiaopeng Zhang, Wenrui Dai, Chenglin Li, Hongkai Xiong, Qi Tian |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2507.20842)<br> [GitHub](https://github.com/YuchenLiu98/METEOR)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.07-red)]() [![Star](https://img.shields.io/github/stars/liaolea/TransPrune.svg?style=social&label=Star)](https://github.com/https://github.com/liaolea/TransPrune)<br>[TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model](https://arxiv.org/abs/2507.20630)<br>Ao Li, Yuxiang Duan, Jinghui Zhang, Congbo Ma, Yutong Xie, Gustavo Carneiro, Mohammad Yaqub, Hu Wang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2507.20630)<br> [GitHub](https://github.com/liaolea/TransPrune)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.07-red)]() [![Star](https://img.shields.io/github/stars/cokeshao/Awesome-Multimodal-Token-Compression.svg?style=social&label=Star)](https://github.com/https://github.com/cokeshao/Awesome-Multimodal-Token-Compression)<br>[When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios](https://arxiv.org/abs/2507.20198)<br>Kele Shao, Keda Tao, Kejia Zhang, Sicheng Feng, Mu Cai, Yuzhang Shang, Haoxuan You, Can Qin, Yang Sui, Huan Wang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() [![Area](https://img.shields.io/badge/Survey-purple)]() |  |  [Paper](https://arxiv.org/abs/2507.20198)<br> [GitHub](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.07-red)]() <br>[Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)<br>Weimin Lyu, Qingqiao Hu, Kehan Qi, Zhan Shi, Wentao Huang, Saumya Gupta, Chao Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2507.14497)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.07-red)]() <br>[Training-free Token Reduction for Vision Mamba](https://arxiv.org/abs/2507.14042)<br>Qiankun Ma, Ziyao Zhang, Chi Su, Jie Chen, Zhen Song, Hairong Zheng, Wen Gao |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2507.14042)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.07-red)]() [![Star](https://img.shields.io/github/stars/dvlab-research/VisionThink.svg?style=social&label=Star)](https://github.com/https://github.com/dvlab-research/VisionThink)<br>[VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning](https://arxiv.org/abs/2507.13348)<br>Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2507.13348)<br> [GitHub](https://github.com/dvlab-research/VisionThink)<br> [Model](https://huggingface.co/collections/Senqiao/visionthink-6878d839fae02a079c9c7bfe)<br> [Dataset](https://huggingface.co/collections/Senqiao/visionthink-6878d839fae02a079c9c7bfe)<br> | 
|  [![Publish](https://img.shields.io/badge/EMNLP_Findings-2024-blue)]() <br>[LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2507.02279)<br>Juntao Liu, Liqiang Niu, Wenchao Chen, Jie Zhou, Fandong Meng |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2507.02279)<br> | 
|  [![Publish](https://img.shields.io/badge/IROS-2025-blue)]() <br>[ToSA: Token Merging with Spatial Awareness](https://arxiv.org/abs/2506.20066)<br>Hsiang-Wei Huang, Wenhao Chai, Kuang-Ming Chen, Cheng-Yen Yang, Jenq-Neng Hwang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2506.20066)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/Theia-4869/CDPruner.svg?style=social&label=Star)](https://github.com/https://github.com/Theia-4869/CDPruner)<br>[Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs](https://arxiv.org/abs/2506.10967)<br>Qizhe Zhang, Mengzhen Liu, Lichen Li, Ming Lu, Yuan Zhang, Junwen Pan, Qi She, Shanghang Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2506.10967)<br> [GitHub](https://github.com/Theia-4869/CDPruner)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() <br>[Generic Token Compression in Multimodal Large Language Models from an Explainability Perspective](https://arxiv.org/abs/2506.01097)<br>Lei Lei, Jie Gu, Xiaokang Ma, Chu Tang, Jingmin Chen, Tong Xu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2506.01097)<br> | 
|  [![Publish](https://img.shields.io/badge/ACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/EffiVLM-Bench/EffiVLM-Bench.svg?style=social&label=Star)](https://github.com/https://github.com/EffiVLM-Bench/EffiVLM-Bench)<br>[EffiVLM-Bench: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Visual-Languge Models](https://arxiv.org/abs/2506.00479)<br>Zekun Wang, Minghua Ma, Zexin Wang, Rongchuan Mu, Liping Shan, Ming Liu, Bing Qin |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Benchmark-purple)]() |  |  [Paper](https://arxiv.org/abs/2506.00479)<br> [GitHub](https://github.com/EffiVLM-Bench/EffiVLM-Bench)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/Tencent/SelfEvolvingAgent.svg?style=social&label=Star)](https://github.com/https://github.com/Tencent/SelfEvolvingAgent/tree/main/VScan)<br>[VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.22654)<br>Ce Zhang, Kaixin Ma, Tianqing Fang, Wenhao Yu, Hongming Zhang, Zhisong Zhang, Yaqi Xie, Katia Sycara, Haitao Mi, Dong Yu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.22654)<br> [GitHub](https://github.com/Tencent/SelfEvolvingAgent/tree/main/VScan)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() <br>[Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization](https://arxiv.org/abs/2505.22038)<br>Kaiyuan Li, Xiaoyue Chen, Chen Gao, Yong Li, Xinlei Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.22038)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/wangqinsi1/2025-ICML-CoreMatching.svg?style=social&label=Star)](https://github.com/https://github.com/wangqinsi1/2025-ICML-CoreMatching)<br>[CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models](https://arxiv.org/abs/2505.19235)<br>Qinsi Wang, Hancheng Ye, Ming-Yu Chung, Yudong Liu, Yueqian Lin, Martin Kuo, Mingyuan Ma, Jianyi Zhang, Yiran Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.19235)<br> [GitHub](https://github.com/wangqinsi1/2025-ICML-CoreMatching)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/xuyang-liu16/Awesome-Token-level-Model-Compression.svg?style=social&label=Star)](https://github.com/https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression)<br>[Shifting AI Efficiency From Model-Centric to Data-Centric Compression](https://arxiv.org/abs/2505.19147)<br>Xuyang Liu, Zichen Wen, Shaobo Wang, Junjie Chen, Zhishan Tao, Yubo Wang, Xiangqi Jin, Chang Zou, Yiyu Wang, Chenfei Liao, Xu Zheng, Honggang Chen, Weijia Li, Xuming Hu, Conghui He, Linfeng Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Position--Paper-purple)]() |  |  [Paper](https://arxiv.org/abs/2505.19147)<br> [GitHub](https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/ZLKong/Awesome-Collection-Token-Reduction.svg?style=social&label=Star)](https://github.com/https://github.com/ZLKong/Awesome-Collection-Token-Reduction)<br>[Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality](https://arxiv.org/abs/2505.18227)<br>Zhenglun Kong, Yize Li, Fanhu Zeng, Lei Xin, Shvat Messica, Xue Lin, Pu Zhao, Manolis Kellis, Hao Tang, Marinka Zitnik |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() [![Area](https://img.shields.io/badge/Position--Paper-purple)]() |  |  [Paper](https://arxiv.org/abs/2505.18227)<br> [GitHub](https://github.com/ZLKong/Awesome-Collection-Token-Reduction)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() <br>[Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/abs/2505.10118)<br>Yangfu Li, Hongjian Zhan, Tianyi Chen, Qi Liu, Yue Lu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.10118)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/ByteDance-Seed/Seed1.5-VL.svg?style=social&label=Star)](https://github.com/https://github.com/ByteDance-Seed/Seed1.5-VL)<br>[Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062)<br>Seed Team |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2505.07062)<br> [GitHub](https://github.com/ByteDance-Seed/Seed1.5-VL)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.04-red)]() [![Star](https://img.shields.io/github/stars/MikeWangWZHL/dymu.svg?style=social&label=Star)](https://github.com/https://github.com/MikeWangWZHL/dymu)<br>[DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/abs/2504.17040)<br>Zhenhailong Wang, Senthil Purushwalkam, Caiming Xiong, Silvio Savarese, Heng Ji, Ran Xu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2504.17040)<br> [GitHub](https://github.com/MikeWangWZHL/dymu)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/orailix/PACT.svg?style=social&label=Star)](https://github.com/https://github.com/orailix/PACT)<br>[PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models](https://arxiv.org/abs/2504.08966)<br>Mohamed Dhouib, Davide Buscaldi, Sonia Vanier, Aymen Shabou |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2504.08966)<br> [GitHub](https://github.com/orailix/PACT)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.04-red)]() <br>[QG-VTC: Question-Guided Visual Token Compression in MLLMs for Efficient VQA](https://arxiv.org/abs/2504.00654)<br>Shuai Li, Jian Xu, Xiao-Hui Li, Chao Deng, Lin-Lin Huang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.00654)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/zwl666666/Skip-Vision.svg?style=social&label=Star)](https://github.com/https://github.com/zwl666666/Skip-Vision)<br>[Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping](https://arxiv.org/abs/2503.21817)<br>Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.21817)<br> [GitHub](https://github.com/zwl666666/Skip-Vision)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/ludc506/InternVL-X.svg?style=social&label=Star)](https://github.com/https://github.com/ludc506/InternVL-X)<br>[InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression](https://arxiv.org/abs/2503.21307)<br>Dongchen Lu, Yuyao Sun, Zilu Zhang, Leping Huang, Jianliang Zeng, Mao Shu, Huo Cao |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.21307)<br> [GitHub](https://github.com/ludc506/InternVL-X)<br> [Model](https://huggingface.co/LLCC506/InternVL-X-8B-HD)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-Omni.svg?style=social&label=Star)](https://github.com/https://github.com/QwenLM/Qwen2.5-Omni)<br>[Qwen2.5-Omni Technical Report](https://arxiv.org/abs/2503.20215)<br>Qwen Team |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.20215)<br> [GitHub](https://github.com/QwenLM/Qwen2.5-Omni)<br> [Model](https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2025-blue)]() <br>[TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model](https://arxiv.org/abs/2503.18278)<br>Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, Bo Yuan |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2503.18278)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() <br>[Growing a Twig to Accelerate Large Vision-Language Models](https://arxiv.org/abs/2503.14075)<br>Zhenwei Shao, Mingyang Wang, Zhou Yu, Wenwen Pan, Yan Yang, Tao Wei, Hongyuan Zhang, Ning Mao, Wei Chen, Jun Yu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.14075)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/ShawnTan86/TokenCarve.svg?style=social&label=Star)](https://github.com/https://github.com/ShawnTan86/TokenCarve)<br>[TokenCarve: Information-Preserving Visual Token Compression in Multimodal Large Language Models](https://arxiv.org/abs/2503.10501)<br>Xudong Tan, Peng Ye, Chongjun Tu, Jianjian Cao, Yaoxin Yang, Lin Zhang, Dongzhan Zhou, Tao Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2503.10501)<br> [GitHub](https://github.com/ShawnTan86/TokenCarve)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/vbdi/divprune.svg?style=social&label=Star)](https://github.com/https://github.com/vbdi/divprune)<br>[DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models](https://arxiv.org/abs/2503.02175)<br>Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, Yong Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2503.02175)<br> [GitHub](https://github.com/vbdi/divprune)<br> | 
|  [![Publish](https://img.shields.io/badge/NAACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/AIoT-MLSys-Lab/MEDA.svg?style=social&label=Star)](https://github.com/https://github.com/AIoT-MLSys-Lab/MEDA)<br>[MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2502.17599)<br>Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2502.17599)<br> [GitHub](https://github.com/AIoT-MLSys-Lab/MEDA)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.02-red)]() [![Star](https://img.shields.io/github/stars/ZichenWen1/DART.svg?style=social&label=Star)](https://github.com/https://github.com/ZichenWen1/DART)<br>[Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More](https://arxiv.org/abs/2502.11494)<br>Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, Linfeng Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2502.11494)<br> [GitHub](https://github.com/ZichenWen1/DART)<br> | 
</details>

<details open>
<summary><strong>Video</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/sihany077/VFlowOpt.svg?style=social&label=Star)](https://github.com/https://github.com/sihany077/VFlowOpt)<br>[VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization](https://arxiv.org/abs/2508.05211)<br>Sihan Yang, Runsen Xu, Chenhang Cui, Tai Wang, Dahua Lin, Jiangmiao Pang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.05211)<br> [GitHub](https://github.com/sihany077/VFlowOpt)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.07-red)]() [![Star](https://img.shields.io/github/stars/cokeshao/Awesome-Multimodal-Token-Compression.svg?style=social&label=Star)](https://github.com/https://github.com/cokeshao/Awesome-Multimodal-Token-Compression)<br>[When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios](https://arxiv.org/abs/2507.20198)<br>Kele Shao, Keda Tao, Kejia Zhang, Sicheng Feng, Mu Cai, Yuzhang Shang, Haoxuan You, Can Qin, Yang Sui, Huan Wang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() [![Area](https://img.shields.io/badge/Survey-purple)]() |  |  [Paper](https://arxiv.org/abs/2507.20198)<br> [GitHub](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.07-red)]() <br>[EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)<br>Jiaao Li, Kaiyuan Li, Chen Gao, Yong Li, Xinlei Chen |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2507.15428)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/HYUNJS/STTM.svg?style=social&label=Star)](https://github.com/https://github.com/HYUNJS/STTM)<br>[Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video-LLMs](https://arxiv.org/abs/2507.07990)<br>Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Joon-Young Lee, Seon Joo Kim, Minho Shim |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2507.07990)<br> [GitHub](https://github.com/HYUNJS/STTM)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.07-red)]() [![Star](https://img.shields.io/github/stars/InternRobotics/StreamVLN.svg?style=social&label=Star)](https://github.com/https://github.com/InternRobotics/StreamVLN)<br>[StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling](https://arxiv.org/abs/2507.05240)<br>Meng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, Xihui Liu, Jiangmiao Pang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2507.05240)<br> [GitHub](https://github.com/InternRobotics/StreamVLN)<br> [Dataset](https://huggingface.co/datasets/cywan/StreamVLN-Trajectory-Data)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() <br>[AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding](https://arxiv.org/abs/2507.02591)<br>Weili Xu, Enxin Song, Wenhao Chai, Xuexiang Wen, Tian Ye, Gaoang Wang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2507.02591)<br> | 
|  [![Publish](https://img.shields.io/badge/EMNLP_Findings-2024-blue)]() <br>[LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2507.02279)<br>Juntao Liu, Liqiang Niu, Wenchao Chen, Jie Zhou, Fandong Meng |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2507.02279)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/HumanMLLM/LLaVA-Scissor.svg?style=social&label=Star)](https://github.com/https://github.com/HumanMLLM/LLaVA-Scissor)<br>[LLaVA-Scissor: Token Compression with Semantic Connected Components for Video-LLMs](https://arxiv.org/abs/2506.21862)<br>Boyuan Sun, Jiaxing Zhao, Xihan Wei, Qibin Hou |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2506.21862)<br> [GitHub](https://github.com/HumanMLLM/LLaVA-Scissor)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/VectorSpaceLab/Video-XL.svg?style=social&label=Star)](https://github.com/https://github.com/VectorSpaceLab/Video-XL)<br>[Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV Sparsification](https://arxiv.org/abs/2506.19225)<br>Minghao Qin, Xiangrui Liu, Zhengyang Liang, Yan Shu, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2506.19225)<br> [GitHub](https://github.com/VectorSpaceLab/Video-XL)<br> [Model](https://huggingface.co/collections/BAAI/video-xl-683973cd45636acda09a11bd)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/Theia-4869/CDPruner.svg?style=social&label=Star)](https://github.com/https://github.com/Theia-4869/CDPruner)<br>[Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs](https://arxiv.org/abs/2506.10967)<br>Qizhe Zhang, Mengzhen Liu, Lichen Li, Ming Lu, Yuan Zhang, Junwen Pan, Qi She, Shanghang Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2506.10967)<br> [GitHub](https://github.com/Theia-4869/CDPruner)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() <br>[DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding](https://arxiv.org/abs/2506.03990)<br>Hongzhi Zhang, Jingyuan Zhang, Xingguang Ji, Qi Wang, Fuzheng Zhang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2506.03990)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/mnyuew/METok.svg?style=social&label=Star)](https://github.com/https://github.com/mnyuew/METok)<br>[METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding](https://arxiv.org/abs/2506.02850)<br>Mengyue Wang, Shuo Chen, Kristian Kersting, Volker Tresp, Yunpu Ma |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2506.02850)<br> [GitHub](https://github.com/mnyuew/METok)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() <br>[Generic Token Compression in Multimodal Large Language Models from an Explainability Perspective](https://arxiv.org/abs/2506.01097)<br>Lei Lei, Jie Gu, Xiaokang Ma, Chu Tang, Jingmin Chen, Tong Xu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2506.01097)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/yunzhuzhang0918/flexselect.svg?style=social&label=Star)](https://github.com/https://github.com/yunzhuzhang0918/flexselect)<br>[FlexSelect: Flexible Token Selection for Efficient Long Video Understanding](https://arxiv.org/abs/2506.00993)<br>Yunzhu Zhang, Yu Lu, Tianyi Wang, Fengyun Rao, Yi Yang, Linchao Zhu |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2506.00993)<br> [GitHub](https://github.com/yunzhuzhang0918/flexselect)<br> | 
|  [![Publish](https://img.shields.io/badge/ACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/EffiVLM-Bench/EffiVLM-Bench.svg?style=social&label=Star)](https://github.com/https://github.com/EffiVLM-Bench/EffiVLM-Bench)<br>[EffiVLM-Bench: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Visual-Languge Models](https://arxiv.org/abs/2506.00479)<br>Zekun Wang, Minghua Ma, Zexin Wang, Rongchuan Mu, Liping Shan, Ming Liu, Bing Qin |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Benchmark-purple)]() |  |  [Paper](https://arxiv.org/abs/2506.00479)<br> [GitHub](https://github.com/EffiVLM-Bench/EffiVLM-Bench)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/Tencent/SelfEvolvingAgent.svg?style=social&label=Star)](https://github.com/https://github.com/Tencent/SelfEvolvingAgent/tree/main/VScan)<br>[VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.22654)<br>Ce Zhang, Kaixin Ma, Tianqing Fang, Wenhao Yu, Hongming Zhang, Zhisong Zhang, Yaqi Xie, Katia Sycara, Haitao Mi, Dong Yu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.22654)<br> [GitHub](https://github.com/Tencent/SelfEvolvingAgent/tree/main/VScan)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/cokeshao/HoliTom.svg?style=social&label=Star)](https://github.com/https://github.com/cokeshao/HoliTom)<br>[HoliTom: Holistic Token Merging for Fast Video Large Language Models](https://arxiv.org/abs/2505.21334)<br>Kele Shao, Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.21334)<br> [GitHub](https://github.com/cokeshao/HoliTom)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() <br>[AdaTP: Attention-Debiased Token Pruning for Video Large Language Models](https://arxiv.org/abs/2505.20100)<br>Fengyuan Sun, Leqi Shen, Hui Chen, Sicheng Zhao, Jungong Han, Guiguang Ding |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.20100)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/wangqinsi1/2025-ICML-CoreMatching.svg?style=social&label=Star)](https://github.com/https://github.com/wangqinsi1/2025-ICML-CoreMatching)<br>[CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models](https://arxiv.org/abs/2505.19235)<br>Qinsi Wang, Hancheng Ye, Ming-Yu Chung, Yudong Liu, Yueqian Lin, Martin Kuo, Mingyuan Ma, Jianyi Zhang, Yiran Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.19235)<br> [GitHub](https://github.com/wangqinsi1/2025-ICML-CoreMatching)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/xuyang-liu16/Awesome-Token-level-Model-Compression.svg?style=social&label=Star)](https://github.com/https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression)<br>[Shifting AI Efficiency From Model-Centric to Data-Centric Compression](https://arxiv.org/abs/2505.19147)<br>Xuyang Liu, Zichen Wen, Shaobo Wang, Junjie Chen, Zhishan Tao, Yubo Wang, Xiangqi Jin, Chang Zou, Yiyu Wang, Chenfei Liao, Xu Zheng, Honggang Chen, Weijia Li, Xuming Hu, Conghui He, Linfeng Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Position--Paper-purple)]() |  |  [Paper](https://arxiv.org/abs/2505.19147)<br> [GitHub](https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/ZLKong/Awesome-Collection-Token-Reduction.svg?style=social&label=Star)](https://github.com/https://github.com/ZLKong/Awesome-Collection-Token-Reduction)<br>[Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality](https://arxiv.org/abs/2505.18227)<br>Zhenglun Kong, Yize Li, Fanhu Zeng, Lei Xin, Shvat Messica, Xue Lin, Pu Zhao, Manolis Kellis, Hao Tang, Marinka Zitnik |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() [![Area](https://img.shields.io/badge/Position--Paper-purple)]() |  |  [Paper](https://arxiv.org/abs/2505.18227)<br> [GitHub](https://github.com/ZLKong/Awesome-Collection-Token-Reduction)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/shilinyan99/CrossLMM.svg?style=social&label=Star)](https://github.com/https://github.com/shilinyan99/CrossLMM)<br>[CrossLMM: Decoupling Long Video Sequences from LMMs via Dual Cross-Attention Mechanisms](https://arxiv.org/abs/2505.17020)<br>Shilin Yan, Jiaming Han, Joey Tsai, Hongwei Xue, Rongyao Fang, Lingyi Hong, Ziyu Guo, Ray Zhang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2505.17020)<br> [GitHub](https://github.com/shilinyan99/CrossLMM)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/xuyang-liu16/VidCom2.svg?style=social&label=Star)](https://github.com/https://github.com/xuyang-liu16/VidCom2)<br>[Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models](https://arxiv.org/abs/2505.14454)<br>Xuyang Liu, Yiyu Wang, Junpeng Ma, Linfeng Zhang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.14454)<br> [GitHub](https://github.com/xuyang-liu16/VidCom2)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/ByteDance-Seed/Seed1.5-VL.svg?style=social&label=Star)](https://github.com/https://github.com/ByteDance-Seed/Seed1.5-VL)<br>[Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062)<br>Seed Team |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2505.07062)<br> [GitHub](https://github.com/ByteDance-Seed/Seed1.5-VL)<br> | 
|  [![Publish](https://img.shields.io/badge/ACM_MM-2025-blue)]() [![Star](https://img.shields.io/github/stars/yaolinli/TimeChat-Online.svg?style=social&label=Star)](https://github.com/https://github.com/yaolinli/TimeChat-Online)<br>[TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/abs/2504.17343)<br>Linli Yao, Yicheng Li, Yuancheng Wei, Lei Li, Shuhuai Ren, Yuanxin Liu, Kun Ouyang, Lean Wang, Shicheng Li, Sida Li, Lingpeng Kong, Qi Liu, Yuanxing Zhang, Xu Sun |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.17343)<br> [GitHub](https://github.com/yaolinli/TimeChat-Online)<br> [Model](https://huggingface.co/wyccccc/TimeChatOnline-7B)<br> [Dataset](https://huggingface.co/datasets/yaolily/TimeChat-Online-139K)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.04-red)]() [![Star](https://img.shields.io/github/stars/MikeWangWZHL/dymu.svg?style=social&label=Star)](https://github.com/https://github.com/MikeWangWZHL/dymu)<br>[DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs](https://arxiv.org/abs/2504.17040)<br>Zhenhailong Wang, Senthil Purushwalkam, Caiming Xiong, Silvio Savarese, Heng Ji, Ran Xu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2504.17040)<br> [GitHub](https://github.com/MikeWangWZHL/dymu)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/orailix/PACT.svg?style=social&label=Star)](https://github.com/https://github.com/orailix/PACT)<br>[PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models](https://arxiv.org/abs/2504.08966)<br>Mohamed Dhouib, Davide Buscaldi, Sonia Vanier, Aymen Shabou |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2504.08966)<br> [GitHub](https://github.com/orailix/PACT)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/steven-ccq/ViLAMP.svg?style=social&label=Star)](https://github.com/https://github.com/steven-ccq/ViLAMP)<br>[Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](https://arxiv.org/abs/2504.02438)<br>Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.02438)<br> [GitHub](https://github.com/steven-ccq/ViLAMP)<br> [Model](https://huggingface.co/orange-sk/ViLAMP-llava-qwen)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/ludc506/InternVL-X.svg?style=social&label=Star)](https://github.com/https://github.com/ludc506/InternVL-X)<br>[InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression](https://arxiv.org/abs/2503.21307)<br>Dongchen Lu, Yuyao Sun, Zilu Zhang, Leping Huang, Jianliang Zeng, Mao Shu, Huo Cao |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.21307)<br> [GitHub](https://github.com/ludc506/InternVL-X)<br> [Model](https://huggingface.co/LLCC506/InternVL-X-8B-HD)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-Omni.svg?style=social&label=Star)](https://github.com/https://github.com/QwenLM/Qwen2.5-Omni)<br>[Qwen2.5-Omni Technical Report](https://arxiv.org/abs/2503.20215)<br>Qwen Team |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.20215)<br> [GitHub](https://github.com/QwenLM/Qwen2.5-Omni)<br> [Model](https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() <br>[SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding](https://arxiv.org/abs/2503.18943)<br>Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, Meng Cao, Kai Kang, Yinfei Yang, Afshin Dehghan |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.18943)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/VectorSpaceLab/Video-XL.svg?style=social&label=Star)](https://github.com/https://github.com/VectorSpaceLab/Video-XL)<br>[Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video Understanding](https://arxiv.org/abs/2503.18478)<br>Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, Bo Zhao |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.18478)<br> [GitHub](https://github.com/VectorSpaceLab/Video-XL)<br> [Model](https://huggingface.co/MINT-SJTU/Video-XL-Pro-3B)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2025-blue)]() <br>[TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model](https://arxiv.org/abs/2503.18278)<br>Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, Bo Yuan |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2503.18278)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() <br>[Long-VMNet: Accelerating Long-Form Video Understanding via Fixed Memory](https://arxiv.org/abs/2503.13707)<br>Saket Gurukar, Asim Kadav |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.13707)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/dvlab-research/LSDBench.svg?style=social&label=Star)](https://github.com/https://github.com/dvlab-research/LSDBench)<br>[Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?](https://arxiv.org/abs/2503.12496)<br>Tianyuan Qu, Longxiang Tang, Bohao Peng, Senqiao Yang, Bei Yu, Jiaya Jia |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Benchmark-purple)]() |  |  [Paper](https://arxiv.org/abs/2503.12496)<br> [GitHub](https://github.com/dvlab-research/LSDBench)<br> [Dataset](https://huggingface.co/datasets/TainU/LSDBench)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/LunarShen/FastVID.svg?style=social&label=Star)](https://github.com/https://github.com/LunarShen/FastVID)<br>[FastVID: Dynamic Density Pruning for Fast Video Large Language Models](https://arxiv.org/abs/2503.11187)<br>Leqi Shen, Guoqiang Gong, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, Guiguang Ding |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2503.11187)<br> [GitHub](https://github.com/LunarShen/FastVID)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() <br>[Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing](https://arxiv.org/abs/2503.10742)<br>Yudong Liu, Jingwei Sun, Yueqian Lin, Jingyang Zhang, Ming Yin, Qinsi Wang, Jianyi Zhang, Hai Li, Yiran Chen |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  |  [Paper](https://arxiv.org/abs/2503.10742)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() <br>[Token-Efficient Long Video Understanding for Multimodal LLMs](https://arxiv.org/abs/2503.04130)<br>Jindong Jiang, Xiuyu Li, Zhijian Liu, Muyang Li, Guo Chen, Zhiqi Li, De-An Huang, Guilin Liu, Zhiding Yu, Kurt Keutzer, Sungjin Ahn, Jan Kautz, Hongxu Yin, Yao Lu, Song Han, Wonmin Byeon |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.04130)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/vbdi/divprune.svg?style=social&label=Star)](https://github.com/https://github.com/vbdi/divprune)<br>[DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models](https://arxiv.org/abs/2503.02175)<br>Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, Yong Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2503.02175)<br> [GitHub](https://github.com/vbdi/divprune)<br> | 
|  [![Publish](https://img.shields.io/badge/NAACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/AIoT-MLSys-Lab/MEDA.svg?style=social&label=Star)](https://github.com/https://github.com/AIoT-MLSys-Lab/MEDA)<br>[MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2502.17599)<br>Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2502.17599)<br> [GitHub](https://github.com/AIoT-MLSys-Lab/MEDA)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.02-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-VL.svg?style=social&label=Star)](https://github.com/https://github.com/QwenLM/Qwen2.5-VL)<br>[Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923)<br>Qwen Team |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2502.13923)<br> [GitHub](https://github.com/QwenLM/Qwen2.5-VL)<br> [Model](https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.02-red)]() [![Star](https://img.shields.io/github/stars/ZichenWen1/DART.svg?style=social&label=Star)](https://github.com/https://github.com/ZichenWen1/DART)<br>[Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More](https://arxiv.org/abs/2502.11494)<br>Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, Linfeng Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2502.11494)<br> [GitHub](https://github.com/ZichenWen1/DART)<br> | 
</details>

<details open>
<summary><strong>Audio</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.07-red)]() [![Star](https://img.shields.io/github/stars/cokeshao/Awesome-Multimodal-Token-Compression.svg?style=social&label=Star)](https://github.com/https://github.com/cokeshao/Awesome-Multimodal-Token-Compression)<br>[When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios](https://arxiv.org/abs/2507.20198)<br>Kele Shao, Keda Tao, Kejia Zhang, Sicheng Feng, Mu Cai, Yuzhang Shang, Haoxuan You, Can Qin, Yang Sui, Huan Wang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() [![Area](https://img.shields.io/badge/Survey-purple)]() |  |  [Paper](https://arxiv.org/abs/2507.20198)<br> [GitHub](https://github.com/cokeshao/Awesome-Multimodal-Token-Compression)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/ZLKong/Awesome-Collection-Token-Reduction.svg?style=social&label=Star)](https://github.com/https://github.com/ZLKong/Awesome-Collection-Token-Reduction)<br>[Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality](https://arxiv.org/abs/2505.18227)<br>Zhenglun Kong, Yize Li, Fanhu Zeng, Lei Xin, Shvat Messica, Xue Lin, Pu Zhao, Manolis Kellis, Hao Tang, Marinka Zitnik |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() [![Area](https://img.shields.io/badge/Position--Paper-purple)]() |  |  [Paper](https://arxiv.org/abs/2505.18227)<br> [GitHub](https://github.com/ZLKong/Awesome-Collection-Token-Reduction)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/yangdongchao/ALMTokenizer.svg?style=social&label=Star)](https://github.com/https://github.com/yangdongchao/ALMTokenizer)<br>[ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for Audio Language Modeling](https://arxiv.org/abs/2504.10344)<br>Dongchao Yang, Songxiang Liu, Haohan Guo, Jiankun Zhao, Yuanyuan Wang, Helin Wang, Zeqian Ju, Xubo Liu, Xueyuan Chen, Xu Tan, Xixin Wu, Helen Meng |  [![Area](https://img.shields.io/badge/Audio--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.10344)<br> [GitHub](https://github.com/yangdongchao/ALMTokenizer)<br> | 
|  [![Publish](https://img.shields.io/badge/ECAI-2025-blue)]() [![Star](https://img.shields.io/github/stars/andylee-24/token-pruning-audio-transformer.svg?style=social&label=Star)](https://github.com/https://github.com/andylee-24/token-pruning-audio-transformer)<br>[Token Pruning in Audio-Transformers: Optimizing Performance and Decoding Patch Importance](https://arxiv.org/abs/2504.01690)<br>Taehan Lee, Hyukjun Lee |  [![Area](https://img.shields.io/badge/Audio--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.01690)<br> [GitHub](https://github.com/andylee-24/token-pruning-audio-transformer)<br> [Model](https://drive.google.com/drive/folders/1cBDXh98m2qDlYLLX3q6xB-gtU1uUtxhK)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-Omni.svg?style=social&label=Star)](https://github.com/https://github.com/QwenLM/Qwen2.5-Omni)<br>[Qwen2.5-Omni Technical Report](https://arxiv.org/abs/2503.20215)<br>Qwen Team |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.20215)<br> [GitHub](https://github.com/QwenLM/Qwen2.5-Omni)<br> [Model](https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e)<br> | 
|  [![Publish](https://img.shields.io/badge/ACL_Findings-2025-blue)]() [![Star](https://img.shields.io/github/stars/JeongHun0716/MMS-LLaMA.svg?style=social&label=Star)](https://github.com/https://github.com/JeongHun0716/MMS-LLaMA)<br>[MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens](https://arxiv.org/abs/2503.11315)<br>Jeong Hun Yeo, Hyeongseop Rha, Se Jin Park, Yong Man Ro |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.11315)<br> [GitHub](https://github.com/JeongHun0716/MMS-LLaMA)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() <br>[Adaptive Audio-Visual Speech Recognition via Matryoshka-Based Multimodal LLMs](https://arxiv.org/abs/2503.06362)<br>Umberto Cappellazzo, Minsu Kim, Stavros Petridis |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.06362)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.02-red)]() [![Star](https://img.shields.io/github/stars/baichuan-inc/Baichuan-Audio.svg?style=social&label=Star)](https://github.com/https://github.com/baichuan-inc/Baichuan-Audio)<br>[Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction](https://arxiv.org/abs/2502.17239)<br>Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, Jianhua Xu, Haoze Sun, Zenan Zhou, Weipeng Chen |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2502.17239)<br> [GitHub](https://github.com/baichuan-inc/Baichuan-Audio)<br> [Model](https://huggingface.co/baichuan-inc/Baichuan-Audio-Instruct)<br> | 
</details>


### Published in Recent Conference/Journal


<details open>
<summary><strong>ICCV 2025</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/sihany077/VFlowOpt.svg?style=social&label=Star)](https://github.com/https://github.com/sihany077/VFlowOpt)<br>[VFlowOpt: A Token Pruning Framework for LMMs with Visual Information Flow-Guided Optimization](https://arxiv.org/abs/2508.05211)<br>Sihan Yang, Runsen Xu, Chenhang Cui, Tai Wang, Dahua Lin, Jiangmiao Pang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.05211)<br> [GitHub](https://github.com/sihany077/VFlowOpt)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/mlvlab/Representation-Shift.svg?style=social&label=Star)](https://github.com/https://github.com/mlvlab/Representation-Shift)<br>[Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/abs/2508.00367)<br>Joonmyung Choi, Sanghyeok Lee, Byungoh Ko, Eunseo Kim, Jihyung Kil, Hyunwoo J. Kim |  [![Area](https://img.shields.io/badge/Vision--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.00367)<br> [GitHub](https://github.com/mlvlab/Representation-Shift)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/YuchenLiu98/METEOR.svg?style=social&label=Star)](https://github.com/https://github.com/YuchenLiu98/METEOR)<br>[METEOR: Multi-Encoder Collaborative Token Pruning for Efficient Vision Language Models](https://arxiv.org/abs/2507.20842)<br>Yuchen Liu, Yaoming Wang, Bowen Shi, Xiaopeng Zhang, Wenrui Dai, Chenglin Li, Hongkai Xiong, Qi Tian |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2507.20842)<br> [GitHub](https://github.com/YuchenLiu98/METEOR)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/HYUNJS/STTM.svg?style=social&label=Star)](https://github.com/https://github.com/HYUNJS/STTM)<br>[Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video-LLMs](https://arxiv.org/abs/2507.07990)<br>Jeongseok Hyun, Sukjun Hwang, Su Ho Han, Taeoh Kim, Inwoong Lee, Dongyoon Wee, Joon-Young Lee, Seon Joo Kim, Minho Shim |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2507.07990)<br> [GitHub](https://github.com/HYUNJS/STTM)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() <br>[AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding](https://arxiv.org/abs/2507.02591)<br>Weili Xu, Enxin Song, Wenhao Chai, Xuexiang Wen, Tian Ye, Gaoang Wang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2507.02591)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/zwl666666/Skip-Vision.svg?style=social&label=Star)](https://github.com/https://github.com/zwl666666/Skip-Vision)<br>[Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping](https://arxiv.org/abs/2503.21817)<br>Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.21817)<br> [GitHub](https://github.com/zwl666666/Skip-Vision)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() <br>[Growing a Twig to Accelerate Large Vision-Language Models](https://arxiv.org/abs/2503.14075)<br>Zhenwei Shao, Mingyang Wang, Zhou Yu, Wenwen Pan, Yan Yang, Tao Wei, Hongyuan Zhang, Ning Mao, Wei Chen, Jun Yu |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.14075)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/dvlab-research/LSDBench.svg?style=social&label=Star)](https://github.com/https://github.com/dvlab-research/LSDBench)<br>[Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?](https://arxiv.org/abs/2503.12496)<br>Tianyuan Qu, Longxiang Tang, Bohao Peng, Senqiao Yang, Bei Yu, Jiaya Jia |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Benchmark-purple)]() |  |  [Paper](https://arxiv.org/abs/2503.12496)<br> [GitHub](https://github.com/dvlab-research/LSDBench)<br> [Dataset](https://huggingface.co/datasets/TainU/LSDBench)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/anakin-skywalker-Joseph/Folder.svg?style=social&label=Star)](https://github.com/https://github.com/anakin-skywalker-Joseph/Folder)<br>[FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance](https://arxiv.org/abs/2501.02430)<br>Haicheng Wang, Zhemeng Yu, Gabriele Spadaro, Chen Ju, Victor Qu√©tu, Shuai Xiao, Enzo Tartaglione |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2501.02430)<br> [GitHub](https://github.com/anakin-skywalker-Joseph/Folder)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/thu-nics/FrameFusion.svg?style=social&label=Star)](https://github.com/https://github.com/thu-nics/FrameFusion)<br>[FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Visual Language Models](https://arxiv.org/abs/2501.01986)<br>Tianyu Fu, Tengxuan Liu, Qinghao Han, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, Yu Wang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() |  [Paper](https://arxiv.org/abs/2501.01986)<br> [GitHub](https://github.com/thu-nics/FrameFusion)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/Hon-Wong/ByteVideoLLM.svg?style=social&label=Star)](https://github.com/https://github.com/Hon-Wong/ByteVideoLLM)<br>[Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM](https://arxiv.org/abs/2412.09530)<br>Han Wang, Yuxiang Nie, Yongjie Ye, Deng GuanYu, Yanjie Wang, Shuai Li, Haiyang Yu, Jinghui Lu, Can Huang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2412.09530)<br> [GitHub](https://github.com/Hon-Wong/ByteVideoLLM)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/dvlab-research/Lyra.svg?style=social&label=Star)](https://github.com/https://github.com/dvlab-research/Lyra)<br>[Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition](https://arxiv.org/abs/2412.09501)<br>Zhisheng Zhong, Chengyao Wang, Yuqi Liu, Senqiao Yang, Longxiang Tang, Yuechen Zhang, Jingyao Li, Tianyuan Qu, Yanwei Li, Yukang Chen, Shaozuo Yu, Sitong Wu, Eric Lo, Shu Liu, Jiaya Jia |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2412.09501)<br> [GitHub](https://github.com/dvlab-research/Lyra)<br> [Model](https://huggingface.co/collections/zszhong/lyra-model-674ea5bb3b39ff8f15de75fc)<br> [Dataset](https://huggingface.co/collections/zszhong/lyra-data-675d80fbab80334eb52cdd82)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/LaVi-Lab/AIM.svg?style=social&label=Star)](https://github.com/https://github.com/LaVi-Lab/AIM)<br>[AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning](https://arxiv.org/abs/2412.03248)<br>Yiwu Zhong, Zhuoming Liu, Yin Li, Liwei Wang |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2412.03248)<br> [GitHub](https://github.com/LaVi-Lab/AIM)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/Theia-4869/VisPruner.svg?style=social&label=Star)](https://github.com/https://github.com/Theia-4869/VisPruner)<br>[Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs](https://arxiv.org/abs/2412.01818)<br>Qizhe Zhang, Aosong Cheng, Ming Lu, Renrui Zhang, Zhiyong Zhuo, Jiajun Cao, Shaobo Guo, Qi She, Shanghang Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2412.01818)<br> [GitHub](https://github.com/Theia-4869/VisPruner)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() <br>[ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification](https://arxiv.org/abs/2410.08584)<br>Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, Bohan Zhuang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2410.08584)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/42Shawn/LLaVA-PruMerge.svg?style=social&label=Star)](https://github.com/https://github.com/42Shawn/LLaVA-PruMerge)<br>[LLaVA-PruMerge:¬†Adaptive Token Reduction for Efficient Large Multimodal Models](https://arxiv.org/abs/2403.15388)<br>Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2403.15388)<br> [GitHub](https://github.com/42Shawn/LLaVA-PruMerge)<br> | 
</details>

<details open>
<summary><strong>ACL 2025</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/EffiVLM-Bench/EffiVLM-Bench.svg?style=social&label=Star)](https://github.com/https://github.com/EffiVLM-Bench/EffiVLM-Bench)<br>[EffiVLM-Bench: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Visual-Languge Models](https://arxiv.org/abs/2506.00479)<br>Zekun Wang, Minghua Ma, Zexin Wang, Rongchuan Mu, Liping Shan, Ming Liu, Bing Qin |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() [![Area](https://img.shields.io/badge/Benchmark-purple)]() |  |  [Paper](https://arxiv.org/abs/2506.00479)<br> [GitHub](https://github.com/EffiVLM-Bench/EffiVLM-Bench)<br> | 
|  [![Publish](https://img.shields.io/badge/ACL_Findings-2025-blue)]() [![Star](https://img.shields.io/github/stars/JeongHun0716/MMS-LLaMA.svg?style=social&label=Star)](https://github.com/https://github.com/JeongHun0716/MMS-LLaMA)<br>[MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens](https://arxiv.org/abs/2503.11315)<br>Jeong Hun Yeo, Hyeongseop Rha, Se Jin Park, Yong Man Ro |  [![Area](https://img.shields.io/badge/Audio--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2503.11315)<br> [GitHub](https://github.com/JeongHun0716/MMS-LLaMA)<br> | 
|  [![Publish](https://img.shields.io/badge/NAACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/AIoT-MLSys-Lab/MEDA.svg?style=social&label=Star)](https://github.com/https://github.com/AIoT-MLSys-Lab/MEDA)<br>[MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2502.17599)<br>Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2502.17599)<br> [GitHub](https://github.com/AIoT-MLSys-Lab/MEDA)<br> | 
|  [![Publish](https://img.shields.io/badge/ACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/Visual-AI/PruneVid.svg?style=social&label=Star)](https://github.com/https://github.com/Visual-AI/PruneVid)<br>[PruneVid: Visual Token Pruning for Efficient Video Large Language Models](https://arxiv.org/abs/2412.16117)<br>Xiaohu Huang, Hao Zhou, Kai Han |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2412.16117)<br> [GitHub](https://github.com/Visual-AI/PruneVid)<br> | 
|  [![Publish](https://img.shields.io/badge/NAACL_Oral-2025-blue)]() [![Star](https://img.shields.io/github/stars/ZongqianLi/Prompt-Compression-Survey.svg?style=social&label=Star)](https://github.com/https://github.com/ZongqianLi/Prompt-Compression-Survey)<br>[Prompt Compression for Large Language Models: A Survey](https://arxiv.org/abs/2410.12388)<br>Zongqian Li, Yinhong Liu, Yixuan Su, Nigel Collier |  [![Area](https://img.shields.io/badge/LLM-purple)]() [![Area](https://img.shields.io/badge/Survey-purple)]() |  |  [Paper](https://arxiv.org/abs/2410.12388)<br> [GitHub](https://github.com/ZongqianLi/Prompt-Compression-Survey)<br> | 
</details>

<details open>
<summary><strong>ICML 2025</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/wangqinsi1/2025-ICML-CoreMatching.svg?style=social&label=Star)](https://github.com/https://github.com/wangqinsi1/2025-ICML-CoreMatching)<br>[CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models](https://arxiv.org/abs/2505.19235)<br>Qinsi Wang, Hancheng Ye, Ming-Yu Chung, Yudong Liu, Yueqian Lin, Martin Kuo, Mingyuan Ma, Jianyi Zhang, Yiran Chen |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2505.19235)<br> [GitHub](https://github.com/wangqinsi1/2025-ICML-CoreMatching)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/yangdongchao/ALMTokenizer.svg?style=social&label=Star)](https://github.com/https://github.com/yangdongchao/ALMTokenizer)<br>[ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for Audio Language Modeling](https://arxiv.org/abs/2504.10344)<br>Dongchao Yang, Songxiang Liu, Haohan Guo, Jiankun Zhao, Yuanyuan Wang, Helin Wang, Zeqian Ju, Xubo Liu, Xueyuan Chen, Xu Tan, Xixin Wu, Helen Meng |  [![Area](https://img.shields.io/badge/Audio--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.10344)<br> [GitHub](https://github.com/yangdongchao/ALMTokenizer)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/steven-ccq/ViLAMP.svg?style=social&label=Star)](https://github.com/https://github.com/steven-ccq/ViLAMP)<br>[Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](https://arxiv.org/abs/2504.02438)<br>Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.02438)<br> [GitHub](https://github.com/steven-ccq/ViLAMP)<br> [Model](https://huggingface.co/orange-sk/ViLAMP-llava-qwen)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/Vision-CAIR/LongVU.svg?style=social&label=Star)](https://github.com/https://github.com/Vision-CAIR/LongVU)<br>[LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding](https://arxiv.org/abs/2410.17434)<br>Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, Vikas Chandra |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2410.17434)<br> [GitHub](https://github.com/Vision-CAIR/LongVU)<br> [Model](https://huggingface.co/collections/Vision-CAIR/longvu-67181d2debabfc1eb050c21d)<br> | 
|  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/Gumpest/SparseVLMs.svg?style=social&label=Star)](https://github.com/https://github.com/Gumpest/SparseVLMs)<br>[SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference](https://arxiv.org/abs/2410.04417)<br>Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Shanghang Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2410.04417)<br> [GitHub](https://github.com/Gumpest/SparseVLMs)<br> | 
</details>

<details open>
<summary><strong>ACM MM 2025</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ACM_MM-2025-blue)]() <br>[Mitigating Information Loss under High Pruning Rates for Efficient Large Vision Language Models](https://arxiv.org/abs/2508.01236)<br>Mingyu Fu, Wei Suo, Ji Ma, Lin Yuanbo Wu, Peng Wang, Yanning Zhang |  [![Area](https://img.shields.io/badge/Image--LLM-purple)]() |  [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.01236)<br> | 
|  [![Publish](https://img.shields.io/badge/ACM_MM-2025-blue)]() [![Star](https://img.shields.io/github/stars/yaolinli/TimeChat-Online.svg?style=social&label=Star)](https://github.com/https://github.com/yaolinli/TimeChat-Online)<br>[TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/abs/2504.17343)<br>Linli Yao, Yicheng Li, Yuancheng Wei, Lei Li, Shuhuai Ren, Yuanxin Liu, Kun Ouyang, Lean Wang, Shicheng Li, Sida Li, Lingpeng Kong, Qi Liu, Yuanxing Zhang, Xu Sun |  [![Area](https://img.shields.io/badge/Video--LLM-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2504.17343)<br> [GitHub](https://github.com/yaolinli/TimeChat-Online)<br> [Model](https://huggingface.co/wyccccc/TimeChatOnline-7B)<br> [Dataset](https://huggingface.co/datasets/yaolily/TimeChat-Online-139K)<br> | 
</details>


---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

This repository is inspired by [Awesome-Efficient-Reasoning-Models](https://github.com/fscdc/Awesome-Efficient-Reasoning-Models), [Awesome-Efficient-LLM](https://github.com/horseee/Awesome-Efficient-LLM/), [Awesome-Context-Engineering](https://github.com/Meirtz/Awesome-Context-Engineering)

## üßë‚Äçüíª Contributors

üëè Thanks to these contributors for this excellent workÔºÅ

<a href="https://github.com/cokeshao/Awesome-Multimodal-Token-Compression/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=cokeshao/Awesome-Multimodal-Token-Compression" />
</a>

## ‚úâÔ∏è Contact

For questions, suggestions, or collaboration opportunities, please feel free to reach out:

‚úâÔ∏è Email:  [shaokele@gmail.com](mailto:shaokele@gmail.com) / [KD.TAO.CT@outlook.com](mailto:KD.TAO.CT@outlook.com)