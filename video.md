
## 2025

*  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/dvlab-research/LSDBench.svg?style=social&label=Star)](https://github.com/dvlab-research/LSDBench) [LSDBench: Long-video Sampling Dilemma Benchmark](https://arxiv.org/abs/2503.12496) [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]()
*  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/HYUNJS/STTM.svg?style=social&label=Star)](https://github.com/HYUNJS/STTM) [Multi-Granular Spatio-Temporal Token Merging for Training-Free Acceleration of Video LLMs](https://arxiv.org/abs/2507.07990)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]()
*  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/rese1f/aurora.svg?style=social&label=Star&color=white)](https://github.com/rese1f/aurora) [AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding](https://arxiv.org/abs/2507.02591) [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/HumanMLLM/LLaVA-Scissor.svg?style=social&label=Star)](https://github.com/HumanMLLM/LLaVA-Scissor) [LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://www.arxiv.org/abs/2506.21862)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/VectorSpaceLab/Video-XL.svg?style=social&label=Star)](https://github.com/VectorSpaceLab/Video-XL/tree/main/Video-XL-2) [Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV Sparsification](https://arxiv.org/abs/2506.19225)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding](https://arxiv.org/abs/2506.03990)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/mnyuew/METok.svg?style=social&label=Star)](https://github.com/mnyuew/METok) [METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding](https://arxiv.org/abs/2506.02850)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.06-red)]() [![Star](https://img.shields.io/github/stars/yunzhuzhang0918/flexselect.svg?style=social&label=Star)](https://github.com/yunzhuzhang0918/flexselect) [FlexSelect: Flexible Token Selection for Efficient Long Video Understanding](https://arxiv.org/abs/2506.00993)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/cokeshao/HoliTom.svg?style=social&label=Star)](https://github.com/cokeshao/HoliTom) [HoliTom: Holistic Token Merging for Fast Video Large Language Models](https://arxiv.org/abs/2505.21334)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [AdaTP: Attention-Debiased Token Pruning for
Video Large Language Models](https://arxiv.org/abs/2505.20100)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/shilinyan99/CrossLMM.svg?style=social&label=Star)](https://github.com/shilinyan99/CrossLMM) [CrossLMM: Decoupling Long Video Sequences from
LMMs via Dual Cross-Attention Mechanisms](https://arxiv.org/abs/2505.17020)
 [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/xuyang-liu16/VidCom2.svg?style=social&label=Star)](https://github.com/xuyang-liu16/VidCom2) [Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models](https://arxiv.org/abs/2505.14454)
 [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.05-red)]() [![Star](https://img.shields.io/github/stars/ByteDance-Seed/Seed1.5-VL.svg?style=social&label=Star)](https://github.com/ByteDance-Seed/Seed1.5-VL) [Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.04-red)]() [![Star](https://img.shields.io/github/stars/yaolinli/TimeChat-Online.svg?style=social&label=Star)](https://github.com/yaolinli/TimeChat-Online) [TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos](https://arxiv.org/abs/2504.17343)
 [![Area](https://img.shields.io/badge/Streaming_Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.04-red)]() [QG-VTC: Question-Guided Visual Token Compression in MLLMs for Efficient VQA](https://arxiv.org/abs/2504.00654)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/ludc506/InternVL-X.svg?style=social&label=Star)](https://github.com/ludc506/InternVL-X) [InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression](https://arxiv.org/abs/2503.21307)
 [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/VectorSpaceLab/Video-XL.svg?style=social&label=Star)](https://github.com/VectorSpaceLab/Video-XL/tree/main/Video-XL-Pro) [Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video Understanding](https://arxiv.org/abs/2503.18478)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [![Star](https://img.shields.io/github/stars/LunarShen/FastVID.svg?style=social&label=Star)](https://github.com/LunarShen/FastVID) [FastVID: Dynamic Density Pruning for Fast Video Large Language Models](https://arxiv.org/abs/2503.11187)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing](https://arxiv.org/abs/2503.10742v1)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [Token-Efficient Long Video Understanding for Multimodal LLMs](https://arxiv.org/abs/2503.04130v1)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
* [![Arxiv](https://img.shields.io/badge/arXiv-2025\.03-red)]() [Long-VMNet: Accelerating Long-Form Video Understanding via Fixed Memory](https://arxiv.org/abs/2503.13707)
[![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]()
*  [![Publish](https://img.shields.io/badge/NAACL-2025-blue)]() [MEDA: Dynamic KV Cache Allocation for Efficient
Multimodal Long-Context Inference](https://arxiv.org/abs/2502.17599)
 [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.02-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen2.5-VL.svg?style=social&label=Star)](https://github.com/QwenLM/Qwen2.5-VL) [Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/ICLR-2025-blue)]() [![Star](https://img.shields.io/github/stars/ictnlp/LLaVA-Mini.svg?style=social&label=Star)](https://github.com/ictnlp/LLaVA-Mini) [LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token](https://arxiv.org/abs/2501.03895)
 [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/thu-nics/FrameFusion.svg?style=social&label=Star)](https://github.com/thu-nics/FrameFusion) [FrameFusion: Combining Similarity and Importance for Video Token Reduction
on Large Visual Language Models](https://arxiv.org/abs/2501.01986)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()
*  [![Publish](https://img.shields.io/badge/ICLR-2025-blue)]() [![Star](https://img.shields.io/github/stars/rese1f/aurora.svg?style=social&label=Star)](https://github.com/rese1f/aurora) [AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark](https://arxiv.org/abs/2410.03051)
 [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()
*  [![Publish](https://img.shields.io/badge/CVPR-2025-blue)]() [Zero-shot 3D Question Answering via Voxel-based Dynamic Token Compression](https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Zero-shot_3D_Question_Answering_via_Voxel-based_Dynamic_Token_Compression_CVPR_2025_paper.pdf)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Area](https://img.shields.io/badge/3D_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]()


## 2024

*  [![Publish](https://img.shields.io/badge/ACL-2025-blue)]() [![Star](https://img.shields.io/github/stars/Visual-AI/PruneVid.svg?style=social&label=Star)](https://github.com/Visual-AI/PruneVid) [PruneVid: Visual Token Pruning for Efficient Video Large Language Models](https://arxiv.org/abs/2412.16117v1)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]()
*  [![Publish](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/OpenGVLab/PVC.svg?style=social&label=Star)](https://github.com/OpenGVLab/PVC) [PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models](https://arxiv.org/abs/2412.09613)
 [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
 *  [![Publish](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/VectorSpaceLab/Video-XL.svg?style=social&label=Star)](https://github.com/VectorSpaceLab/Video-XL) [Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding](https://arxiv.org/abs/2409.14485)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/Hon-Wong/ByteVideoLLM.svg?style=social&label=Star)](https://github.com/Hon-Wong/ByteVideoLLM) [Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM](https://arxiv.org/abs/2412.09530)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/CVPR-2025-blue)]() [![Star](https://img.shields.io/github/stars/KD-TAO/DyCoke?tab=readme-ov-file.svg?style=social&label=Star)](https://github.com/KD-TAO/DyCoke?tab=readme-ov-file) [DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models](https://arxiv.org/abs/2411.15024)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]()
*  [![Publish](https://img.shields.io/badge/CVPR_Highlight-2025-blue)]() [AdaCM2: Adaptive Crossâ€‘Modality Memory Reduction](https://arxiv.org/abs/2411.12593)
 [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/ICML-2025-blue)]() [![Star](https://img.shields.io/github/stars/Vision-CAIR/LongVU.svg?style=social&label=Star)](https://github.com/Vision-CAIR/LongVU) [LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding](https://arxiv.org/abs/2410.17434)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
* [![Arxiv](https://img.shields.io/badge/arXiv-2024\.12-red)]()  [![Star](https://img.shields.io/github/stars/gls0425/LinVT.svg?style=social&label=Star)](https://github.com/ziplab/LongVLM) [LinVT: Empower Your Image-level Large Language Model to Understand Videos](https://arxiv.org/abs/2412.05185)
[![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
* [![Arxiv](https://img.shields.io/badge/arXiv-2024\.10-red)]() [xGen-MM-Vid (BLIP-3-Video):An Efficient Video-Language Model](https://arxiv.org/abs/2410.16267)
[![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.10-red)]() [![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star)](https://github.com/LLaVA-VL/LLaVA-NeXT) [Video Instruction Tuning with Synthetic Data](http://arxiv.org/abs/2410.02713)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.09-red)]() [![Star](https://img.shields.io/github/stars/QwenLM/Qwen2-VL.svg?style=social&label=Star)](https://github.com/QwenLM/Qwen2-VL) [Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](https://arxiv.org/abs/2409.12191)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.08-red)]() [![Star](https://img.shields.io/github/stars/LLaVA-VL/LLaVA-NeXT.svg?style=social&label=Star)](https://github.com/LLaVA-VL/LLaVA-NeXT) [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.07-red)]() [![Star](https://img.shields.io/github/stars/apple/ml-slowfast-llava.svg?style=social&label=Star)](https://github.com/apple/ml-slowfast-llava) [SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models](https://arxiv.org/abs/2407.15841)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.06-red)]() [![Star](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA2.svg?style=social&label=Star)](https://github.com/DAMO-NLP-SG/VideoLLaMA2) [VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs](https://arxiv.org/abs/2406.07476)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.04-red)]() [![Star](https://img.shields.io/github/stars/magic-research/PLLaVA.svg?style=social&label=Star)](https://github.com/magic-research/PLLaVA) [PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning](https://arxiv.org/abs/2404.16994)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/ECCV-2024-blue)]() [![Star](https://img.shields.io/github/stars/ziplab/LongVLM.svg?style=social&label=Star)](https://github.com/ziplab/LongVLM) [LongVLM: Efficient Long Video Understanding via Large Language Models](https://arxiv.org/abs/2404.03384)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()

## 2023

*  [![Publish](https://img.shields.io/badge/ECCV-2024-blue)]() [![Star](https://img.shields.io/github/stars/dvlab-research/LLaMA-VID.svg?style=social&label=Star)](https://github.com/dvlab-research/LLaMA-VID) [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/CVPR_Highlight-2024-blue)]() [![Star](https://img.shields.io/github/stars/PKU-YuanGroup/Chat-UniVi.svg?style=social&label=Star)](https://github.com/PKU-YuanGroup/Chat-UniVi) [Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding](https://arxiv.org/abs/2311.08046)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/CVPR-2024-blue)]() [![Star](https://img.shields.io/github/stars/rese1f/MovieChat.svg?style=social&label=Star)](https://github.com/rese1f/MovieChat) [MovieChat: From Dense Token to Sparse Memory for Long Video Understanding](https://arxiv.org/abs/2307.16449)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Publish](https://img.shields.io/badge/ACL-2024-blue)]() [![Star](https://img.shields.io/github/stars/mbzuai-oryx/Video-ChatGPT.svg?style=social&label=Star)](https://github.com/mbzuai-oryx/Video-ChatGPT) [Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models](https://arxiv.org/abs/2306.05424v2)
 [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
*  [![Arxiv](https://img.shields.io/badge/arXiv-2023\.06-red)]() [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858)
 [![Area](https://img.shields.io/badge/Audio_LLM-purple)]() [![Area](https://img.shields.io/badge/Image_LLM-purple)]() [![Area](https://img.shields.io/badge/Video_LLM-purple)]() [![Type](https://img.shields.io/badge/Query--Based-green)]() [![Cost](https://img.shields.io/badge/Training--Based-yellow)]()
