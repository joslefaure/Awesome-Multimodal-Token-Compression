
<details open>
<summary><strong>2025 ViT</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[TCSAFormer: Efficient Vision Transformer with Token Compression and Sparse Attention for Medical Image Segmentation](https://arxiv.org/abs/2508.04058)<br>Zunhui Xia, Hongxing Li, Libin Lan |  [![Area](https://img.shields.io/badge/Vision--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]() [![Type](https://img.shields.io/badge/Similarity--Based-green)]() [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.04058)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.08-red)]() <br>[Neutralizing Token Aggregation via Information Augmentation for Efficient Test-Time Adaptation](https://arxiv.org/abs/2508.03388)<br>Yizhe Xiong, Zihan Zhou, Yiwen Liang, Hui Chen, Zijia Lin, Tianxiang Hao, Fan Zhang, Jungong Han, Guiguang Ding |  [![Area](https://img.shields.io/badge/Vision--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2508.03388)<br> | 
|  [![Publish](https://img.shields.io/badge/ICCV-2025-blue)]() [![Star](https://img.shields.io/github/stars/mlvlab/Representation-Shift.svg?style=social&label=Star)](https://github.com/https://github.com/mlvlab/Representation-Shift)<br>[Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/abs/2508.00367)<br>Joonmyung Choi, Sanghyeok Lee, Byungoh Ko, Eunseo Kim, Jihyung Kil, Hyunwoo J. Kim |  [![Area](https://img.shields.io/badge/Vision--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2508.00367)<br> [GitHub](https://github.com/mlvlab/Representation-Shift)<br> | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2025\.07-red)]() <br>[ToFe: Lagged Token Freezing and Reusing for Efficient Vision Transformer Inference](https://arxiv.org/abs/2507.16260)<br>Haoyue Zhang, Jie Zhang, Song Guo |  [![Area](https://img.shields.io/badge/Vision--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2507.16260)<br> | 
</details>

<details open>
<summary><strong>2024 ViT</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Arxiv](https://img.shields.io/badge/arXiv-2024\.05-red)]() [![Star](https://img.shields.io/github/stars/yaolinli/DeCo.svg?style=social&label=Star)](https://github.com/https://github.com/yaolinli/DeCo)<br>[DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models](https://arxiv.org/abs/2405.20985)<br>Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, Lu Hou |  [![Area](https://img.shields.io/badge/Vision--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Transformation--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2405.20985)<br> [GitHub](https://github.com/yaolinli/DeCo)<br> | 
|  [![Publish](https://img.shields.io/badge/CVPR-2024-blue)]() [![Star](https://img.shields.io/github/stars/double125/MADTP-plus.svg?style=social&label=Star)](https://github.com/https://github.com/double125/MADTP-plus)<br>[MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer](https://arxiv.org/abs/2403.02991)<br>Jianjian Cao, Peng Ye, Shengze Li, Chong Yu, Yansong Tang, Jiwen Lu, Tao Chen |  [![Area](https://img.shields.io/badge/Vision--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Attention--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2403.02991)<br> [GitHub](https://github.com/double125/MADTP-plus)<br> | 
</details>

<details open>
<summary><strong>2023 ViT</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ACL-2023-blue)]() [![Star](https://img.shields.io/github/stars/csarron/PuMer.svg?style=social&label=Star)](https://github.com/https://github.com/csarron/PuMer)<br>[PuMer: Pruning and Merging Tokens for Efficient Vision Language Models](https://arxiv.org/abs/2305.17530)<br>Qingqing Cao, Bhargavi Paranjape, Hannaneh Hajishirzi |  [![Area](https://img.shields.io/badge/Vision--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2305.17530)<br> [GitHub](https://github.com/csarron/PuMer)<br> | 
</details>

<details open>
<summary><strong>2022 ViT</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ICLR_Oral-2023-blue)]() [![Star](https://img.shields.io/github/stars/facebookresearch/ToMe.svg?style=social&label=Star)](https://github.com/https://github.com/facebookresearch/ToMe)<br>[Token Merging: Your ViT But Faster](https://arxiv.org/abs/2210.09461)<br>Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, Judy Hoffman |  [![Area](https://img.shields.io/badge/Vision--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Similarity--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() [![Cost](https://img.shields.io/badge/Training--Free-yellow)]() |  [Paper](https://arxiv.org/abs/2210.09461)<br> [GitHub](https://github.com/facebookresearch/ToMe)<br> | 
</details>

<details open>
<summary><strong>2021 ViT</strong></summary>

| **Title & Authors** | **Areas** | **Tags** | **Links** | 
| --- | --- | --- | :---: | 
|  [![Publish](https://img.shields.io/badge/ICML-2021-blue)]() <br>[Perceiver: General Perception with Iterative Attention](https://arxiv.org/abs/2103.03206)<br>Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, Joao Carreira |  [![Area](https://img.shields.io/badge/Vision--Transformer-purple)]() |  [![Type](https://img.shields.io/badge/Query--Based-green)]()<br> [![Cost](https://img.shields.io/badge/Training--Based-yellow)]() |  [Paper](https://arxiv.org/abs/2103.03206)<br> | 
</details>
